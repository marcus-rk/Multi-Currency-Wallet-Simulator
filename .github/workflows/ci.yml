name: CI

# CI workflow goals:
# - Catch style/quality issues early (lint)
# - Verify business rules quickly (unit tests)
# - Verify system wiring across layers (integration tests)
# - Publish code-quality signals to SonarCloud (coverage + analysis)
#
# Trigger on push/PR to main so changes are validated before merge.
on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  lint:
    name: Lint
    runs-on: ubuntu-latest

    steps:
      # Pull repository contents into the runner.
      - name: Checkout code
        uses: actions/checkout@v4

      # Use a consistent Python version (and enable pip caching).
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      # Install project deps + the lint tool.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi
          python -m pip install pylint

      # Run static analysis and capture output.
      # We keep the normal log output, but also save a copy as an artifact so it can
      # be downloaded for offline review/debugging.
      - name: Lint with Pylint (capture output)
        shell: bash
        run: |
          set -o pipefail
          python -m pylint app --fail-under=8.0 2>&1 | tee lint_output.txt

      - name: Upload lint output artifact
        # Upload even when lint fails so the output is still available.
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: lint-output
          path: lint_output.txt

  unit-tests:
    name: Unit tests
    runs-on: ubuntu-latest

    steps:
      # Pull repository contents into the runner.
      - name: Checkout code
        uses: actions/checkout@v4

      # Use a consistent Python version (and enable pip caching).
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      # Install project deps + test runner.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi
          python -m pip install pytest pytest-cov

      # Run unit tests for fast feedback on domain/service logic.
      # Output is captured to a file and uploaded as an artifact for easy download.
      - name: Run unit tests (with coverage, capture output)
        shell: bash
        run: |
          set -o pipefail
          python -m pytest tests/unit --cov=app --cov-report=term-missing 2>&1 | tee unit_output.txt

      - name: Upload unit test output artifact
        # Upload even when tests fail so the output is still available.
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-output
          path: unit_output.txt

  integration-tests:
    name: Integration tests
    runs-on: ubuntu-latest

    steps:
      # Pull repository contents into the runner.
      - name: Checkout code
        uses: actions/checkout@v4

      # Use a consistent Python version (and enable pip caching).
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      # Install project deps + test runner.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi
          python -m pip install pytest pytest-cov

      # Run vertical-slice checks (HTTP -> services -> repos -> SQLite).
      # Output is captured to a file and uploaded as an artifact for easy download.
      - name: Run integration tests (capture output)
        shell: bash
        run: |
          set -o pipefail
          python -m pytest -m integration 2>&1 | tee integration_output.txt

      - name: Upload integration output artifact
        # Upload even when tests fail so the output is still available.
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-output
          path: integration_output.txt

  sonarcloud:
    name: SonarCloud
    needs: [lint, unit-tests, integration-tests]
    runs-on: ubuntu-latest

    steps:
      # Pull repository contents into the runner.
      # Full history improves SonarCloud PR decoration/blame information.
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # Use a consistent Python version (and enable pip caching).
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: 'pip'

      # Install deps + tooling needed to generate coverage.xml for SonarCloud.
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then python -m pip install -r requirements.txt; fi
          python -m pip install pytest pytest-cov

      # Generate a Cobertura XML report that SonarCloud can import.
      - name: Run unit tests (coverage.xml)
        run: |
          python -m pytest tests/unit --cov=app --cov-report=xml:coverage.xml

      # Sanity check: make sure coverage.xml was created.
      - name: Debug coverage.xml exists
        run: |
          ls -la
          ls -la coverage.xml

      # Upload analysis to SonarCloud (requires SONAR_TOKEN secret).
      - name: SonarCloud scan
        uses: SonarSource/sonarqube-scan-action@v7
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
